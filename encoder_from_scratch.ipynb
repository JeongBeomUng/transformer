{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df881251-349b-498d-98dd-c03ca7a39cf6",
   "metadata": {},
   "source": [
    "# Transformer Encoder from scratch using PyTorch\n",
    "## 목표: self attention을 직접 구현하여 입력 문장의 문맥을 포함하는 벡터 표현 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "024a3dd5-45b0-4881-826b-5ec5084527aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63b25e8f-e3cc-407b-a4ab-e94faec6198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'My', 1: 'name', 2: 'is', 3: 'Beomung', 4: 'Jeong', 5: 'I', 6: 'really', 7: 'love', 8: 'computer', 9: 'vision'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"My name is Beomung Jeong. I really love computer vision.\"\n",
    "tokens=input_text.replace('.','').split(' ')\n",
    "vocab = {idx:token for idx, token in enumerate(tokens)}\n",
    "print(vocab)\n",
    "\n",
    "encoder_input=torch.tensor([[0,1,2,3,4],[5,6,7,8,9]])\n",
    "encoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3001d-f3be-474f-a2d1-c01e57a1edd4",
   "metadata": {},
   "source": [
    "## 토큰 임베딩 & 위치 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30baf29a-7cf1-47ec-9f29-b8e61cba0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self,x):\n",
    "        return (self.embed_dim**0.5) * self.embedding(x)   # 토큰 임베딩이 포지셔널 인코딩에 비하여 값이 너무 작지 않도록 스케일링.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, len_max_seq , embed_dim):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(len_max_seq ,embed_dim)\n",
    "        for pos in range(len_max_seq):\n",
    "            for i in range(embed_dim//2):\n",
    "                pe[pos,2*i]   = math.sin( pos / (10000**((2*i)/embed_dim)) )\n",
    "                pe[pos,2*i+1] = math.cos( pos / (10000**((2*i)/embed_dim)) )\n",
    "        pe = pe.unsqueeze(0)            # 배치 차원 추가\n",
    "        self.register_buffer('pe', pe)  # 포지셔널 인코딩은 학습하지 않으므로 레지스터 버퍼로 등록한다. 부모 클래스(nn.Module)의 메서드.\n",
    "        \n",
    "    def forward(self, len_seq):\n",
    "        return self.pe[:,:len_seq,:]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a7b5e-1d85-4812-9360-34fbc05cbbb1",
   "metadata": {},
   "source": [
    "## 멀티 헤드 어텐션\n",
    "클래스 하나에 모든 헤드의 셀프 어텐션을 한 번에 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8adaff-065b-4ccb-8b1b-ca9a494079c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = embed_dim//num_heads\n",
    "        self.w_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.w_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(embed_dim,embed_dim)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        d_k = Q.size(-1)\n",
    "        return torch.matmul( torch.softmax( torch.matmul(Q,K.transpose(-2,-1))/d_k**0.5 , dim=-1 ) , V )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        len_seq    = x.shape[1]\n",
    "        # transpose(1,2): 쿼리, 키, 밸류의 마지막 두 차원이 (시퀀스 길이 x 헤드 하나의 차원)이 되도록 사용.\n",
    "        Q = self.w_q(x).view(batch_size, len_seq, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.w_k(x).view(batch_size, len_seq, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        V = self.w_v(x).view(batch_size, len_seq, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        x = self.scaled_dot_product_attention(Q,K,V)\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().view(batch_size,len_seq,self.embed_dim)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139658b-acdd-40ed-a546-5149604ddd6b",
   "metadata": {},
   "source": [
    "## 레이어 정규화\n",
    "임베딩차원의 방향으로 레이어 정규화를 진행하는 클래스이다.\n",
    "torch의 브로드캐스팅 기능을 사용하는 대신 직접 텐서의 차원을 맞추는 과정을 포함하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305ca720-d35e-4b1f-a382-22702cc5b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))        # 스케일링 인자 초기화\n",
    "        self.beta  = nn.Parameter(torch.zeros(embed_dim))       # 시프팅 인자 초기화\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        len_seq    = x.shape[1]\n",
    "        embed_dim  = x.shape[2]\n",
    "        \n",
    "        # expand를 사용하여 텐서의 차원을 동일하게 변경\n",
    "        mean  = x.mean(dim=-1,keepdim=True).expand(batch_size, len_seq, embed_dim)\n",
    "        var   = x.var(dim=-1,keepdim=True).expand(batch_size, len_seq, embed_dim)\n",
    "        gamma = self.gamma.expand(batch_size, len_seq, embed_dim)\n",
    "        beta  = self.beta.expand(batch_size, len_seq, embed_dim)\n",
    "        \n",
    "        return gamma * ((x-mean)/(torch.sqrt(var)+self.eps)) + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b2065-9bbe-4937-bb51-f27e81f17d8e",
   "metadata": {},
   "source": [
    "## 피드 포워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecff669-59cd-4311-a60e-ab36fb86dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07906298-30cb-4efa-a8f1-d446207ca7f1",
   "metadata": {},
   "source": [
    "## 인코더 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53064216-af17-42ce-8de4-d02eaf1660ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ffn = FeedForward(embed_dim, hidden_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ln1 = LayerNorm(embed_dim)\n",
    "        self.ln2 = LayerNorm(embed_dim)\n",
    "        self.do1 = nn.Dropout(0.1)\n",
    "        self.do2 = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.do1(self.mha(x)))\n",
    "        x = self.ln2(x + self.do2(self.ffn(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a6d08-8004-439b-8e1d-7788c92f2038",
   "metadata": {},
   "source": [
    "## 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5793d7d2-bf02-43e5-a59d-428d8601d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, len_max_seq, embed_dim, hidden_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding     = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(len_max_seq, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(embed_dim, hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        len_seq    = x.shape[1]\n",
    "        x = self.token_embedding(x) + self.positional_encoding(len_seq)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b45fc-0564-48c6-85fc-7704869e7fc0",
   "metadata": {},
   "source": [
    "## 인코더 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89604efa-bab0-4f20-bc97-995fcdcf505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder=TransformerEncoder(vocab_size  = len(vocab),\n",
    "                                       len_max_seq = 5,\n",
    "                                       embed_dim   = 128,\n",
    "                                       hidden_dim  = 512,\n",
    "                                       num_heads   = 2,\n",
    "                                       num_layers  = 5\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadeb52c-2b32-4508-90e7-05cd2ac67012",
   "metadata": {},
   "source": [
    "## 인코더 입력 & 인코더 출력 텐서 모양 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d11abb90-0916-48d6-8794-700b26aeb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder_output=transformer_encoder(encoder_input)\n",
    "print(encoder_input.shape)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78697ba-e381-4223-933a-493eaa5cc72f",
   "metadata": {},
   "source": [
    "## 인코더 출력 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4570b7a7-d0c7-45e4-8a1c-ba48fcb3f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9442,  1.1392,  0.1584,  ..., -0.9235,  0.3670, -0.7178],\n",
       "         [-0.5536,  0.1302, -0.9800,  ..., -0.5950,  0.9755, -0.4821],\n",
       "         [ 0.2435, -0.7550, -0.5374,  ..., -1.3884, -0.7751,  0.0983],\n",
       "         [ 0.5625,  0.5783, -0.3128,  ..., -0.7499, -0.9429, -0.7940],\n",
       "         [-1.4342,  0.4418,  0.1311,  ..., -0.1247,  0.5048, -0.3715]],\n",
       "\n",
       "        [[-1.1368,  1.3688,  2.2270,  ...,  0.1017, -0.6633, -0.0391],\n",
       "         [ 1.1998,  1.4955,  1.3791,  ...,  0.4756,  0.5808, -0.4179],\n",
       "         [-0.9269,  0.6578,  0.9291,  ..., -0.3846,  0.3989,  0.0327],\n",
       "         [-0.4421,  0.8831, -0.2456,  ..., -1.2086, -0.3106, -0.4489],\n",
       "         [-0.8949, -0.1022,  1.7923,  ...,  0.5330, -0.6286, -0.8362]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
